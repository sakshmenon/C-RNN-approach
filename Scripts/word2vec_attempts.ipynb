{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#include <stdio.h>\\n\\nint main(void)\\n{\\n    int result = 1;\\n    if (result == 1)\\n    {\\n        printf(\"Executing critical code...\\\\n\");\\n    }\\n    else\\n    {\\n        printf(\"Exiting out...\\\\n\");\\n        return 1;\\n    }\\n\\n    return 0;\\n}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.chdir(\"/home/sakshmeno/Documents/GitHub/C-RNN-approach/Labels\")\n",
    "os.chdir(\"/Users/saksh.menon/Documents/GitHub/C-RNN-approach/manual_dataset\")\n",
    "with open(\"guillermo_branch_simple_insecure.c\") as dataset_obj:\n",
    "    codeLines = dataset_obj.read()\n",
    "\n",
    "codeLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_preprocessing(file):\n",
    "    with open(file) as dataset_obj:\n",
    "        codeLines = dataset_obj.read()\n",
    "    comment_lines = []\n",
    "    raw_codeLines = codeLines.replace(\"\\t\",\"\").split(\"\\n\")\n",
    "    multi_line_flag = 0\n",
    "    for line_number in range(len(raw_codeLines)):\n",
    "        if multi_line_flag:\n",
    "            if raw_codeLines[line_number].__contains__(\"*/\"):\n",
    "                multi_line_flag = 0\n",
    "            comment_lines.append(line_number)\n",
    "        elif raw_codeLines[line_number].__contains__(\"/*\"):\n",
    "            if raw_codeLines[line_number].startswith(\"/*\") and not(raw_codeLines[line_number].__contains__(\"*/\")):\n",
    "                comment_lines.append(line_number)\n",
    "                multi_line_flag = 1\n",
    "            elif raw_codeLines[line_number].__contains__(\"/*\") and not(raw_codeLines[line_number].startswith(\"/*\")):\n",
    "                if raw_codeLines[line_number].__contains__(\"*/\"):\n",
    "                    psuedo_multi_line_start = raw_codeLines[line_number].find(\"/*\")\n",
    "                    psuedo_multi_line_end = raw_codeLines[line_number].find(\"*/\")\n",
    "                    temporary_line = raw_codeLines[line_number][:psuedo_multi_line_start] + raw_codeLines[line_number][psuedo_multi_line_end+2:]\n",
    "                    raw_codeLines[line_number] = temporary_line\n",
    "\n",
    "        elif raw_codeLines[line_number].startswith(\"//\"):\n",
    "            comment_lines.append(line_number)\n",
    "        elif raw_codeLines[line_number].__contains__(\"//\"):\n",
    "            comment_start = raw_codeLines[line_number].find('//')\n",
    "            raw_codeLines[line_number] = raw_codeLines[line_number][:comment_start]\n",
    "            \n",
    "    comment_lines.reverse()\n",
    "    for i in comment_lines:\n",
    "        raw_codeLines.pop(i)\n",
    "\n",
    "    def insert_space(string, index):\n",
    "        string_copy = \"\"\n",
    "        for i in range(len(string)):\n",
    "            if i==(index):\n",
    "                string_copy += \" \"\n",
    "                string_copy += string[i]\n",
    "                string_copy += \" \"\n",
    "                continue\n",
    "            string_copy += string[i]\n",
    "        return string_copy\n",
    "\n",
    "    def find_char_indices(input_string, char):\n",
    "        indices = []\n",
    "        replacement_token = 0\n",
    "        for index, character in enumerate(input_string):\n",
    "            if character == char:\n",
    "                indices.append(index + 2*replacement_token)\n",
    "                replacement_token+=1\n",
    "        return indices\n",
    "\n",
    "    def space_out(string, char):\n",
    "        indices = find_char_indices(string, char)\n",
    "\n",
    "        for i in indices:\n",
    "            string = insert_space(string, i)\n",
    "        return string\n",
    "\n",
    "    # for line_number in range(len(raw_codeLines)):\n",
    "    #     placeHolder = raw_codeLines[line_number]\n",
    "        # placeHolder = space_out(placeHolder, \";\")\n",
    "        # placeHolder = space_out(placeHolder, \"(\")\n",
    "        # placeHolder = space_out(placeHolder, \")\")\n",
    "        # placeHolder = space_out(placeHolder, \",\")\n",
    "        # if placeHolder.endswith(\";\"):\n",
    "        #     raw_codeLines[line_number] = \"<start> \" + placeHolder.replace(\";\",\"<end>\")\n",
    "        # elif not(placeHolder.endswith(\";\")):\n",
    "        #     raw_codeLines[line_number] = \"<start> \" + placeHolder + \" <end>\"\n",
    "\n",
    "    return raw_codeLines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = code_preprocessing('guillermo_branch_simple_insecure.c')\n",
    "f = open('FILTERED_DATASET.txt','w')\n",
    "for i in range(len(dataset)):\n",
    "    f.write(dataset[i])\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(\"/Users/saksh.menon/Documents/GitHub/C-RNN-approach/manual_dataset/FILTERED_DATASET.txt\").filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', '1', 'return', 'result', 'int', 'stdioh', 'printfexiting', 'printfexecuting', 'outn', 'mainvoid', 'include', 'if', 'else', 'critical', 'coden', '0']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11  6  0  0  0  0  0  0  0  0] \t=>\t ['include', 'stdioh', '', '', '', '', '', '', '', '']\n",
      "[ 5 10  0  0  0  0  0  0  0  0] \t=>\t ['int', 'mainvoid', '', '', '', '', '', '', '', '']\n",
      "[0 0 0 0 0 0 0 0 0 0] \t=>\t ['', '', '', '', '', '', '', '', '', '']\n",
      "[5 4 2 0 0 0 0 0 0 0] \t=>\t ['int', 'result', '1', '', '', '', '', '', '', '']\n",
      "[12  4  2  0  0  0  0  0  0  0] \t=>\t ['if', 'result', '1', '', '', '', '', '', '', '']\n",
      "[0 0 0 0 0 0 0 0 0 0] \t=>\t ['', '', '', '', '', '', '', '', '', '']\n",
      "[ 8 14 15  0  0  0  0  0  0  0] \t=>\t ['printfexecuting', 'critical', 'coden', '', '', '', '', '', '', '']\n",
      "[0 0 0 0 0 0 0 0 0 0] \t=>\t ['', '', '', '', '', '', '', '', '', '']\n",
      "[13  0  0  0  0  0  0  0  0  0] \t=>\t ['else', '', '', '', '', '', '', '', '', '']\n",
      "[0 0 0 0 0 0 0 0 0 0] \t=>\t ['', '', '', '', '', '', '', '', '', '']\n",
      "[7 9 0 0 0 0 0 0 0 0] \t=>\t ['printfexiting', 'outn', '', '', '', '', '', '', '', '']\n",
      "[3 2 0 0 0 0 0 0 0 0] \t=>\t ['return', '1', '', '', '', '', '', '', '', '']\n",
      "[0 0 0 0 0 0 0 0 0 0] \t=>\t ['', '', '', '', '', '', '', '', '', '']\n",
      "[ 3 16  0  0  0  0  0  0  0  0] \t=>\t ['return', '0', '', '', '', '', '', '', '', '']\n",
      "[0 0 0 0 0 0 0 0 0 0] \t=>\t ['', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences:\n",
    "  print(f\"{seq} \\t=>\\t {[inverse_vocab[i] for i in seq]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
