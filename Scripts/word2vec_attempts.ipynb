{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 12:31:01.675573: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-23 12:31:01.718556: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-23 12:31:01.718591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-23 12:31:01.719695: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-23 12:31:01.725351: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-23 12:31:01.725829: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-23 12:31:02.704510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/sakshmeno/Documents/GitHub/C-RNN-approach/manual_dataset\")\n",
    "# os.chdir(\"/Users/saksh.menon/Documents/GitHub/C-RNN-approach/manual_dataset\")\n",
    "# with open(\"guillermo_branch_simple_insecure.c\") as dataset_obj:\n",
    "#     codeLines = dataset_obj.read()\n",
    "\n",
    "# codeLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_preprocessing(file):\n",
    "    with open(file) as dataset_obj:\n",
    "        codeLines = dataset_obj.read()\n",
    "    comment_lines = []\n",
    "    raw_codeLines = codeLines.replace(\"\\t\",\"\").split(\"\\n\")\n",
    "    multi_line_flag = 0\n",
    "    for line_number in range(len(raw_codeLines)):\n",
    "        if multi_line_flag:\n",
    "            if raw_codeLines[line_number].__contains__(\"*/\"):\n",
    "                multi_line_flag = 0\n",
    "            comment_lines.append(line_number)\n",
    "        elif raw_codeLines[line_number].__contains__(\"/*\"):\n",
    "            if raw_codeLines[line_number].startswith(\"/*\") and not(raw_codeLines[line_number].__contains__(\"*/\")):\n",
    "                comment_lines.append(line_number)\n",
    "                multi_line_flag = 1\n",
    "            elif raw_codeLines[line_number].__contains__(\"/*\") and not(raw_codeLines[line_number].startswith(\"/*\")):\n",
    "                if raw_codeLines[line_number].__contains__(\"*/\"):\n",
    "                    psuedo_multi_line_start = raw_codeLines[line_number].find(\"/*\")\n",
    "                    psuedo_multi_line_end = raw_codeLines[line_number].find(\"*/\")\n",
    "                    temporary_line = raw_codeLines[line_number][:psuedo_multi_line_start] + raw_codeLines[line_number][psuedo_multi_line_end+2:]\n",
    "                    raw_codeLines[line_number] = temporary_line\n",
    "\n",
    "        elif raw_codeLines[line_number].startswith(\"//\"):\n",
    "            comment_lines.append(line_number)\n",
    "        elif raw_codeLines[line_number].__contains__(\"//\"):\n",
    "            comment_start = raw_codeLines[line_number].find('//')\n",
    "            raw_codeLines[line_number] = raw_codeLines[line_number][:comment_start]\n",
    "            \n",
    "    comment_lines.reverse()\n",
    "    for i in comment_lines:\n",
    "        raw_codeLines.pop(i)\n",
    "\n",
    "    def insert_space(string, index):\n",
    "        string_copy = \"\"\n",
    "        for i in range(len(string)):\n",
    "            if i==(index):\n",
    "                string_copy += \" \"\n",
    "                string_copy += string[i]\n",
    "                string_copy += \" \"\n",
    "                continue\n",
    "            string_copy += string[i]\n",
    "        return string_copy\n",
    "\n",
    "    def find_char_indices(input_string, char):\n",
    "        indices = []\n",
    "        replacement_token = 0\n",
    "        for index, character in enumerate(input_string):\n",
    "            if character == char:\n",
    "                indices.append(index + 2*replacement_token)\n",
    "                replacement_token+=1\n",
    "        return indices\n",
    "\n",
    "    def space_out(string, char):\n",
    "        indices = find_char_indices(string, char)\n",
    "\n",
    "        for i in indices:\n",
    "            string = insert_space(string, i)\n",
    "        return string\n",
    "\n",
    "    for line_number in range(len(raw_codeLines)):\n",
    "        placeHolder = raw_codeLines[line_number]\n",
    "        placeHolder = space_out(placeHolder, \";\")\n",
    "        placeHolder = space_out(placeHolder, \"(\")\n",
    "        placeHolder = space_out(placeHolder, \")\")\n",
    "        placeHolder = space_out(placeHolder, \",\")\n",
    "        if placeHolder.endswith(\";\"):\n",
    "            raw_codeLines[line_number] = \"<start> \" + placeHolder.replace(\";\",\"<end>\")\n",
    "        elif not(placeHolder.endswith(\";\")):\n",
    "            raw_codeLines[line_number] = \"<start> \" + placeHolder + \" <end>\"\n",
    "\n",
    "    return raw_codeLines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = code_preprocessing('guillermo_branch_simple_insecure.c')\n",
    "f = open('FILTERED_DATASET.txt','w')\n",
    "for i in range(len(dataset)):\n",
    "    f.write(dataset[i])\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset(\"/home/sakshmeno/Documents/GitHub/C-RNN-approach/manual_dataset/FILTERED_DATASET.txt\").filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'start', 'end', '1', 'return', 'result', 'printf', 'int', 'void', 'stdioh', 'outn', 'main', 'include', 'if', 'exiting', 'executing', 'else', 'critical', 'coden']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  \n",
      "1  :  [UNK]\n",
      "2  :  start\n",
      "3  :  end\n",
      "4  :  1\n",
      "5  :  return\n",
      "6  :  result\n",
      "7  :  printf\n",
      "8  :  int\n",
      "9  :  void\n",
      "10  :  stdioh\n",
      "11  :  outn\n",
      "12  :  main\n",
      "13  :  include\n",
      "14  :  if\n",
      "15  :  exiting\n",
      "16  :  executing\n",
      "17  :  else\n",
      "18  :  critical\n",
      "19  :  coden\n",
      "20  :  0\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(inverse_vocab):\n",
    "    print(i[0], \" : \", i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start', 'include', 'stdioh', 'end', '', '', '', '', '', '']\n",
      "\n",
      "[ 2 13 10  3  0  0  0  0  0  0]\n",
      "\n",
      "['start', 'end', '', '', '', '', '', '', '', '']\n",
      "\n",
      "[2 3 0 0 0 0 0 0 0 0]\n",
      "\n",
      "['start', 'int', 'main', 'void', 'end', '', '', '', '', '']\n",
      "\n",
      "[ 2  8 12  9  3  0  0  0  0  0]\n",
      "\n",
      "['start', 'end', '', '', '', '', '', '', '', '']\n",
      "\n",
      "[2 3 0 0 0 0 0 0 0 0]\n",
      "\n",
      "['start', 'int', 'result', '1', 'end', '', '', '', '', '']\n",
      "\n",
      "[2 8 6 4 3 0 0 0 0 0]\n",
      "\n",
      "['start', 'if', 'result', '1', 'end', '', '', '', '', '']\n",
      "\n",
      "[ 2 14  6  4  3  0  0  0  0  0]\n",
      "\n",
      "['start', 'end', '', '', '', '', '', '', '', '']\n",
      "\n",
      "[2 3 0 0 0 0 0 0 0 0]\n",
      "\n",
      "['start', 'printf', 'executing', 'critical', 'coden', 'end', '', '', '', '']\n",
      "\n",
      "[ 2  7 16 18 19  3  0  0  0  0]\n",
      "\n",
      "['start', 'end', '', '', '', '', '', '', '', '']\n",
      "\n",
      "[2 3 0 0 0 0 0 0 0 0]\n",
      "\n",
      "['start', 'else', 'end', '', '', '', '', '', '', '']\n",
      "\n",
      "[ 2 17  3  0  0  0  0  0  0  0]\n",
      "\n",
      "['start', 'end', '', '', '', '', '', '', '', '']\n",
      "\n",
      "[2 3 0 0 0 0 0 0 0 0]\n",
      "\n",
      "['start', 'printf', 'exiting', 'outn', 'end', '', '', '', '', '']\n",
      "\n",
      "[ 2  7 15 11  3  0  0  0  0  0]\n",
      "\n",
      "['start', 'return', '1', 'end', '', '', '', '', '', '']\n",
      "\n",
      "[2 5 4 3 0 0 0 0 0 0]\n",
      "\n",
      "['start', 'end', '', '', '', '', '', '', '', '']\n",
      "\n",
      "[2 3 0 0 0 0 0 0 0 0]\n",
      "\n",
      "['start', 'end', '', '', '', '', '', '', '', '']\n",
      "\n",
      "[2 3 0 0 0 0 0 0 0 0]\n",
      "\n",
      "['start', 'return', '0', 'end', '', '', '', '', '', '']\n",
      "\n",
      "[ 2  5 20  3  0  0  0  0  0  0]\n",
      "\n",
      "['start', 'end', '', '', '', '', '', '', '', '']\n",
      "\n",
      "[2 3 0 0 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences:\n",
    "  print([inverse_vocab[i] for i in seq])\n",
    "  print()\n",
    "  print(seq)\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
